<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ZFS | Was ich so treibe...]]></title>
  <link href="http://uli-heller.github.com/blog/categories/zfs/atom.xml" rel="self"/>
  <link href="http://uli-heller.github.com/"/>
  <updated>2013-08-07T10:00:49+02:00</updated>
  <id>http://uli-heller.github.com/</id>
  <author>
    <name><![CDATA[Uli Heller]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Experimente mit ZFS unter Ubuntu-12.04]]></title>
    <link href="http://uli-heller.github.com/blog/2013/07/12/zfs/"/>
    <updated>2013-07-12T08:00:00+02:00</updated>
    <id>http://uli-heller.github.com/blog/2013/07/12/zfs</id>
    <content type="html"><![CDATA[<h2>DEBs installieren</h2>

<p>{% codeblock lang:sh %}
sudo apt-get install make build-essential dpkg-dev
sudo apt-get install -f
sudo dpkg -i zfs-dkms_0.6.1-1~precise_amd64.deb \
  spl-dkms_0.6.1-1~precise_all.deb              \
  dkms_2.2.0.3-1ubuntu3.1+zfs6~precise1_all.deb
sudo dpkg -i zfsutils_0.6.1-1~precise_amd64.deb \
  libnvpair1_0.6.1-1~precise_amd64.deb          \
  libuutil1_0.6.1-1~precise_amd64.deb           \
  libzfs1_0.6.1-1~precise_amd64.deb             \
  libzpool1_0.6.1-1~precise_amd64.deb
sudo dpkg -i ubuntu-zfs_7~precise1_amd64.deb    \
  spl_0.6.1-1~precise_amd64.deb
sudo dpkg -i mountall_2.48build1-zfs2_amd64.deb
{% endcodeblock %}</p>

<h2>Kurzexperimente mit ZPOOL</h2>

<h4>Pool anlegen</h4>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata /dev/sda
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  sda       ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
$ sudo zpool destroy zfsdata
{% endcodeblock %}</p>

<h3>Pool mit RaidZ anlegen</h3>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata raidz /dev/sda /dev/sdb
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sda     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
$ sudo zpool destroy zfsdata
{% endcodeblock %}</p>

<h3>Pool und Reboot</h3>

<p>Dieser Test soll sicherstellen, dass ein ZPOOL auch nach einem Reboot verfügbar ist.</p>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata raidz /dev/sda /dev/sdb
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sda     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
$ df|grep zfs
zfsdata                  2873622144    128 2873622016   1% /zfsdata
$ sudo reboot
&hellip;
$ df|grep zfs
zfsdata                  2873622144    128 2873622016   1% /zfsdata
$ sudo zpool destroy zfsdata
{% endcodeblock %}</p>

<p>Das funktioniert offenbar nur dann, wenn das &ldquo;mountall&rdquo;-Paket installiert ist!</p>

<h3>Pool mit RaidZ und Spare Disk (manuell)</h3>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata raidz /dev/sda /dev/sdb spare /dev/sdc
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sda     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
spares
  sdc       AVAIL   
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<p>Jetzt habe ich die Platte /dev/sda rausgezogen. Ideal wäre nun, wenn sich das RAIDZ
automatisch auf /dev/sdc verlagern würde&hellip;</p>

<p>{% codeblock lang:sh %}
$ sudo zpool scrub zfsdata
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices could not be used because the label is missing or</p>

<pre><code>invalid.  Sufficient replicas exist for the pool to continue
functioning in a degraded state.
</code></pre>

<p>action: Replace the device using &lsquo;zpool replace&rsquo;.
   see: <a href="http://zfsonlinux.org/msg/ZFS-8000-4J">http://zfsonlinux.org/msg/ZFS-8000-4J</a>
  scan: scrub repaired 0 in 0h0m with 0 errors on Sat Jul 20 11:07:11 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     DEGRADED     0     0     0
  raidz1-0  DEGRADED     0     0     0
    sda     UNAVAIL      0     0     0  corrupted data
    sdb     ONLINE       0     0     0
spares
  sdc       AVAIL   
</code></pre>

<p>errors: No known data errors
$ sudo zpool replace zfsdata sda sdc
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices could not be used because the label is missing or</p>

<pre><code>invalid.  Sufficient replicas exist for the pool to continue
functioning in a degraded state.
</code></pre>

<p>action: Replace the device using &lsquo;zpool replace&rsquo;.
   see: <a href="http://zfsonlinux.org/msg/ZFS-8000-4J">http://zfsonlinux.org/msg/ZFS-8000-4J</a>
  scan: resilvered 532K in 0h0m with 0 errors on Sat Jul 20 11:12:19 2013
config:</p>

<pre><code>NAME         STATE     READ WRITE CKSUM
zfsdata      DEGRADED     0     0     0
  raidz1-0   DEGRADED     0     0     0
    spare-0  UNAVAIL      0     0     0
      sda    UNAVAIL      0     0     0  corrupted data
      sdc    ONLINE       0     0     0
    sdb      ONLINE       0     0     0
spares
  sdc        INUSE     currently in use
</code></pre>

<p>errors: No known data errors
$ sudo zpool detach zfsdata sda
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: resilvered 532K in 0h0m with 0 errors on Sat Jul 20 11:12:19 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sdc     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<p>Jetzt habe ich /dev/sda wieder eingebaut und mit <code>fdisk</code> geprüft, ob sie auch ansprechbar ist.</p>

<p>{% codeblock lang:sh %}
$ sudo zpool add zfsdata spare sda
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: resilvered 532K in 0h0m with 0 errors on Sat Jul 20 11:12:19 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sdc     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
spares
  sda       AVAIL   
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<h3>Pool mit RaidZ und Spare Disk (automatisch)</h3>

<p>Nachfolgend meine Versuche mit &ldquo;autoreplace&rdquo; &ndash; leider wenig erfolgreich.
Hier <a href="http://forums.nas4free.org/viewtopic.php?f=66&amp;t=3873">http://forums.nas4free.org/viewtopic.php?f=66&amp;t=3873</a> werden die
gleichen Probleme mit FreeBSD beschrieben. Anscheinend unterstützt FreeBSD
dieses Feature nicht richtig, vermutlich ist&rsquo;s bei Linux ebenso.</p>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata raidz /dev/sda /dev/sdb spare /dev/sdc
$ sudo zpool set autoreplace=on zfsdata
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sda     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
spares
  sdc       AVAIL   
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<p>Jetzt habe ich die Platte /dev/sda rausgezogen. Ideal wäre nun, wenn sich das RAIDZ
automatisch auf /dev/sdc verlagern würde&hellip;</p>

<p>{% codeblock lang:sh %}
$ sudo zpool scrub zfsdata
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices has been removed by the administrator.</p>

<pre><code>Sufficient replicas exist for the pool to continue functioning in a
degraded state.
</code></pre>

<p>action: Online the device using &lsquo;zpool online&rsquo; or replace the device with</p>

<pre><code>'zpool replace'.
</code></pre>

<p>  scan: scrub repaired 0 in 0h0m with 0 errors on Sat Jul 20 11:25:59 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     DEGRADED     0     0     0
  raidz1-0  DEGRADED     0     0     0
    sda     REMOVED      0     0     0
    sdb     ONLINE       0     0     0
spares
  sdc       AVAIL   
</code></pre>

<p>errors: No known data errors
$ sudo zpool replace zfsdata sda sdc
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will</p>

<pre><code>continue to function, possibly in a degraded state.
</code></pre>

<p>action: Wait for the resilver to complete.
  scan: resilver in progress since Sat Jul 20 11:33:09 2013</p>

<pre><code>146M scanned out of 1,03G at 8,61M/s, 0h1m to go
72,9M resilvered, 13,94% done
</code></pre>

<p>config:</p>

<pre><code>NAME         STATE     READ WRITE CKSUM
zfsdata      DEGRADED     0     0     0
  raidz1-0   DEGRADED     0     0     0
    spare-0  UNAVAIL      0     0     0
      sda    UNAVAIL      0     0     0  corrupted data
      sdc    ONLINE       0     0     0  (resilvering)
    sdb      ONLINE       0     0     0
spares
  sdc        INUSE     currently in use
</code></pre>

<p>errors: No known data errors
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices could not be used because the label is missing or</p>

<pre><code>invalid.  Sufficient replicas exist for the pool to continue
functioning in a degraded state.
</code></pre>

<p>action: Replace the device using &lsquo;zpool replace&rsquo;.
   see: <a href="http://zfsonlinux.org/msg/ZFS-8000-4J">http://zfsonlinux.org/msg/ZFS-8000-4J</a>
  scan: resilvered 526M in 0h1m with 0 errors on Sat Jul 20 11:34:28 2013
config:</p>

<pre><code>NAME         STATE     READ WRITE CKSUM
zfsdata      DEGRADED     0     0     0
  raidz1-0   DEGRADED     0     0     0
    spare-0  UNAVAIL      0     0     0
      sda    UNAVAIL      0     0     0  corrupted data
      sdc    ONLINE       0     0     0
    sdb      ONLINE       0     0     0
spares
  sdc        INUSE     currently in use
</code></pre>

<p>errors: No known data errors
$ sudo zpool detach zfsdata sda
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: resilvered 526M in 0h1m with 0 errors on Sat Jul 20 11:34:28 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sdc     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<p>Jetzt habe ich /dev/sda wieder eingebaut und mit <code>fdisk</code> geprüft, ob sie auch ansprechbar ist.</p>

<p>{% codeblock lang:sh %}
$ sudo zpool add zfsdata spare sda
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: resilvered 526M in 0h1m with 0 errors on Sat Jul 20 11:34:28 2013
config:</p>

<pre><code>NAME        STATE     READ WRITE CKSUM
zfsdata     ONLINE       0     0     0
  raidz1-0  ONLINE       0     0     0
    sdc     ONLINE       0     0     0
    sdb     ONLINE       0     0     0
spares
  sda       AVAIL   
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<h2>Platten-IDs ermitteln</h2>

<p>{% codeblock lang:sh %}
$ ls /dev/disk/by-id/scsi-SATA_WDC*|grep -v part
/dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0755994
/dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511
/dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577
/dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780</p>

<p>$ ls /dev/disk/by-id/scsi-SATA_WDC*|grep -v part|xargs &mdash;verbose -n1 udevadm info -q path -n
udevadm info -q path -n /dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0755994
/devices/pci0000:00/0000:00:11.0/host3/target3:0:0/3:0:0:0/block/sdd
udevadm info -q path -n /dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511
/devices/pci0000:00/0000:00:11.0/host2/target2:0:0/2:0:0:0/block/sdc
udevadm info -q path -n /dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577
/devices/pci0000:00/0000:00:11.0/host0/target0:0:0/0:0:0:0/block/sda
udevadm info -q path -n /dev/disk/by-id/scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780
/devices/pci0000:00/0000:00:11.0/host1/target1:0:0/1:0:0:0/block/sdb
{% endcodeblock %}</p>

<p>Aus den Ausgaben kann man diese Zuordnung erkennen:</p>

<ul>
<li>sda &ndash; scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577</li>
<li>sdb &ndash; scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780</li>
<li>sdc &ndash; scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511</li>
<li>sdd &ndash; scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0755994</li>
</ul>


<p>Die Platten-IDs sollten am besten auch auf den jeweiligen Einbaurahmen geschrieben
werden, damit es zu keinen Verwechslungen kommen kann.</p>

<h2>ZPool über Platten-IDs</h2>

<p>{% codeblock lang:sh %}
$ sudo zpool create -f zfsdata raidz scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577 \
   scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780 \
   spare scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME                                           STATE     READ WRITE CKSUM
zfsdata                                        ONLINE       0     0     0
  raidz1-0                                     ONLINE       0     0     0
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577  ONLINE       0     0     0
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780  ONLINE       0     0     0
spares
  scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511    AVAIL   
</code></pre>

<p>errors: No known data errors
$ sudo zpool scrub zfsdata
$ sudo zpool status
  pool: zfsdata
 state: DEGRADED
status: One or more devices could not be used because the label is missing or</p>

<pre><code>invalid.  Sufficient replicas exist for the pool to continue
functioning in a degraded state.
</code></pre>

<p>action: Replace the device using &lsquo;zpool replace&rsquo;.
   see: <a href="http://zfsonlinux.org/msg/ZFS-8000-4J">http://zfsonlinux.org/msg/ZFS-8000-4J</a>
  scan: scrub repaired 0 in 0h0m with 0 errors on Sat Jul 20 12:19:07 2013
config:</p>

<pre><code>NAME                                           STATE     READ WRITE CKSUM
zfsdata                                        DEGRADED     0     0     0
  raidz1-0                                     DEGRADED     0     0     0
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577  UNAVAIL      0     0     0  corrupted data
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780  ONLINE       0     0     0
spares
  scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511    AVAIL   
</code></pre>

<p>errors: No known data errors</p>

<p>$ sudo zpool replace zfsdata scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577 scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511
$ sudo zpool detach zfsdata scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: resilvered 532K in 0h0m with 0 errors on Sat Jul 20 12:19:36 2013
config:</p>

<pre><code>NAME                                           STATE     READ WRITE CKSUM
zfsdata                                        ONLINE       0     0     0
  raidz1-0                                     ONLINE       0     0     0
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511  ONLINE       0     0     0
    scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780  ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<h2>Finale</h2>

<p>{% codeblock lang:sh %}
$ sudo zpool create -o ashift=12 -f zfsdata scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577 scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780 scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511
$ sudo zpool status
  pool: zfsdata
 state: ONLINE
  scan: none requested
config:</p>

<pre><code>NAME                                         STATE     READ WRITE CKSUM
zfsdata                                      ONLINE       0     0     0
  scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0771577  ONLINE       0     0     0
  scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0777780  ONLINE       0     0     0
  scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0770511  ONLINE       0     0     0
</code></pre>

<p>errors: No known data errors
{% endcodeblock %}</p>

<p>Die Platte &ldquo;scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0755994&rdquo; bleibt auf Vorrat
nicht eingesteckt im Rahmen. Beim Ausfall einer Platte wird sie eingesteckt und
mittels <code>zpool replace zfsdata {kaputte_platte} scsi-SATA_WDC_WD30EFRX-68_WD-WCC1T0755994</code>
aktiviert.</p>

<h2>Probleme</h2>

<h3>Dateisystem nach Neustart nicht eingebunden</h3>

<p>Nachdem ich den Kernel aktualisiert habe von 3.2 nach 3.5 klappt das automatische
Einhängen des ZFS-Dateisystems nicht mehr.</p>

<p>Abhilfe: Standard-Paket &ldquo;mountall&rdquo; wieder installieren und ZFS umkonfigurieren!</p>

<p>{% codeblock lang:sh %}
sudo apt-get install mountall=2.36.4
sudo sed -i s/ZFS_MOUNT=&lsquo;no&rsquo;/ZFS_MOUNT=&lsquo;yes&rsquo;/ /etc/default/zfs
{% endcodeblock %}</p>

<h2>Änderungen</h2>

<h3>2013-07-20</h3>

<ul>
<li>Formatierung korrigiert</li>
<li>Reboot-Test</li>
<li>Tests mit Spare Disk</li>
<li>Tests mit Platten-IDs</li>
<li>Finale</li>
</ul>

]]></content>
  </entry>
  
</feed>
